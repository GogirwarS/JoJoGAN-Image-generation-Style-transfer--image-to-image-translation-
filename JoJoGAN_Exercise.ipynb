{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gsyBOLdV-7t6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "from torch import nn, autograd, optim\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jDdJDnPXLltx"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iKa7MyqOLo27"
      },
      "outputs": [],
      "source": [
        "# JoJoGAN Specific Import\n",
        "from model import *\n",
        "from e4e_projection import projection as e4e_projection\n",
        "from util import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D-yjnw96Mfot"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-NJp6JXZ_EaV"
      },
      "outputs": [],
      "source": [
        "os.makedirs('inversion_codes', exist_ok=True)\n",
        "os.makedirs('style_images', exist_ok=True)\n",
        "os.makedirs('style_images_aligned', exist_ok=True)\n",
        "os.makedirs('models', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmE31otO_H1A",
        "outputId": "0fa8cd22-d4d1-4f54-e39a-ee33a268946f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-08-02 00:30:35--  http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64040097 (61M)\n",
            "Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’\n",
            "\n",
            "shape_predictor_68_ 100%[===================>]  61.07M  13.0MB/s    in 6.6s    \n",
            "\n",
            "2022-08-02 00:30:42 (9.26 MB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Download models\n",
        "\n",
        "!wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "!bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "!mv shape_predictor_68_face_landmarks.dat models/dlibshape_predictor_68_face_landmarks.dat\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZoZ_3EWm0xP",
        "outputId": "30867d61-e9fb-41e1-ce53-a2cb3b894d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 96M\n",
            "drwxr-xr-x  2 root root 4.0K Aug  2 00:30 .\n",
            "drwxr-xr-x 12 root root 4.0K Aug  2 00:30 ..\n",
            "-rw-r--r--  1 root root  96M Jul 24  2015 dlibshape_predictor_68_face_landmarks.dat\n"
          ]
        }
      ],
      "source": [
        "!ls -lah /content/JoJoGAN/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0GUcs1UC_OJo"
      },
      "outputs": [],
      "source": [
        "drive_ids = {\n",
        "    \"stylegan2-ffhq-config-f.pt\": \"1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\",\n",
        "    \"e4e_ffhq_encode.pt\": \"1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\",\n",
        "    \"restyle_psp_ffhq_encode.pt\": \"1nbxCIVw9H3YnQsoIPykNEFwWJnHVHlVd\",\n",
        "    \"arcane_caitlyn.pt\": \"1gOsDTiTPcENiFOrhmkkxJcTURykW1dRc\",\n",
        "    \"arcane_caitlyn_preserve_color.pt\": \"1cUTyjU-q98P75a8THCaO545RTwpVV-aH\",\n",
        "    \"arcane_jinx_preserve_color.pt\": \"1jElwHxaYPod5Itdy18izJk49K1nl4ney\",\n",
        "    \"arcane_jinx.pt\": \"1quQ8vPjYpUiXM4k1_KIwP4EccOefPpG_\",\n",
        "    \"arcane_multi_preserve_color.pt\": \"1enJgrC08NpWpx2XGBmLt1laimjpGCyfl\",\n",
        "    \"arcane_multi.pt\": \"15V9s09sgaw-zhKp116VHigf5FowAy43f\",\n",
        "    \"sketch_multi.pt\": \"1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\",\n",
        "    \"disney.pt\": \"1zbE2upakFUAx8ximYnLofFwfT8MilqJA\",\n",
        "    \"disney_preserve_color.pt\": \"1Bnh02DjfvN_Wm8c4JdOiNV4q9J7Z_tsi\",\n",
        "    \"jojo.pt\": \"13cR2xjIBj8Ga5jMO7gtxzIJj2PDsBYK4\",\n",
        "    \"jojo_preserve_color.pt\": \"1ZRwYLRytCEKi__eT2Zxv1IlV6BGVQ_K2\",\n",
        "    \"jojo_yasuho.pt\": \"1grZT3Gz1DLzFoJchAmoj3LoM9ew9ROX_\",\n",
        "    \"jojo_yasuho_preserve_color.pt\": \"1SKBu1h0iRNyeKBnya_3BBmLr4pkPeg_L\",\n",
        "    \"art.pt\": \"1a0QDEHwXQ6hE_FcYEyNMuv5r5UnRQLKT\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aldyZAfxLIjV"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijdbXxI5L1Qs"
      },
      "outputs": [],
      "source": [
        "# #@markdown You may optionally enable downloads with pydrive in order to authenticate and avoid drive download limits.\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npiAZn-B_nMa"
      },
      "outputs": [],
      "source": [
        "# from StyelGAN-NADA\n",
        "class GDrive_Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "        \n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "    \n",
        "    def download_file(self, file_name):\n",
        "        file_dst = os.path.join('models', file_name)\n",
        "        file_id = drive_ids[file_name]\n",
        "        if not os.path.exists(file_dst):\n",
        "            print(f'Downloading {file_name}')\n",
        "            if self.use_pydrive:\n",
        "                downloaded = self.drive.CreateFile({'id':file_id})\n",
        "                downloaded.FetchMetadata(fetch_all=True)\n",
        "                downloaded.GetContentFile(file_dst)\n",
        "            else:\n",
        "                !gdown --id $file_id -O $file_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ueu6SogPMGsX"
      },
      "outputs": [],
      "source": [
        "#downloader = Downloader(download_with_pydrive)\n",
        "downloader = GDrive_Downloader()\n",
        "downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
        "downloader.download_file('e4e_ffhq_encode.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "850JdZsUC1_M"
      },
      "outputs": [],
      "source": [
        "# from StyelGAN-NADA\n",
        "class Downloader(object):\n",
        "    def download_file(self, file_name):\n",
        "        file_dst = os.path.join('models', file_name)\n",
        "        file_id = drive_ids[file_name]\n",
        "        if not os.path.exists(file_dst):\n",
        "            print(f'Downloading {file_name}')\n",
        "            !gdown --id $file_id -O $file_dst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcpcdOOwME3T",
        "outputId": "0f7433fa-6a4c-4d87-bfd0-1609ce478896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading stylegan2-ffhq-config-f.pt\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Yr7KuD959btpmcKGAUsbAk5rPjX2MytK\n",
            "To: /content/JoJoGAN/models/stylegan2-ffhq-config-f.pt\n",
            "100% 381M/381M [00:06<00:00, 62.1MB/s]\n",
            "Downloading e4e_ffhq_encode.pt\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1o6ijA3PkcewZvwJJ73dJ0fxhndn0nnh7\n",
            "To: /content/JoJoGAN/models/e4e_ffhq_encode.pt\n",
            "100% 1.20G/1.20G [00:24<00:00, 49.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "downloader = Downloader()\n",
        "downloader.download_file('stylegan2-ffhq-config-f.pt')\n",
        "downloader.download_file('e4e_ffhq_encode.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZQY88lbmr9L",
        "outputId": "de5326e1-09c8-4790-a2c2-369196913bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1.6G\n",
            "drwxr-xr-x  2 root root 4.0K Aug  2 00:31 .\n",
            "drwxr-xr-x 12 root root 4.0K Aug  2 00:30 ..\n",
            "-rw-r--r--  1 root root  96M Jul 24  2015 dlibshape_predictor_68_face_landmarks.dat\n",
            "-rw-r--r--  1 root root 1.2G Aug  2 00:31 e4e_ffhq_encode.pt\n",
            "-rw-r--r--  1 root root 364M Aug  2 00:31 stylegan2-ffhq-config-f.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -lah /content/JoJoGAN/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RZYENyq2MBIv"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' #@param ['cuda', 'cpu']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gpY8qirH_ZGU"
      },
      "outputs": [],
      "source": [
        "latent_dim = 512\n",
        "\n",
        "# Load original generator\n",
        "original_generator = Generator(1024, latent_dim, 8, 2).to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "original_generator.load_state_dict(ckpt[\"g_ema\"], strict=False)\n",
        "mean_latent = original_generator.mean_latent(10000)\n",
        "\n",
        "# to be finetuned generator\n",
        "generator = deepcopy(original_generator)\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((1024, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gm8yId8MzMC",
        "outputId": "540339ba-3bd8-4a8e-e63a-73875d20951e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading sketch_multi.pt\n",
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1GdaeHGBGjBAFsWipTL0y-ssUiAqk8AxD\n",
            "To: /content/JoJoGAN/models/sketch_multi.pt\n",
            "100% 133M/133M [00:02<00:00, 48.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "plt.rcParams['figure.dpi'] = 150\n",
        "pretrained = 'sketch_multi' #@param ['art', 'arcane_multi', 'sketch_multi', 'arcane_jinx', 'arcane_caitlyn', 'jojo_yasuho', 'jojo', 'disney']\n",
        "#@markdown Preserve color tries to preserve color of original image by limiting family of allowable transformations. Otherwise, the stylized image will inherit the colors of the reference images, leading to heavier stylizations.\n",
        "preserve_color = True #@param{type:\"boolean\"}\n",
        "\n",
        "if preserve_color:\n",
        "    ckpt = f'{pretrained}_preserve_color.pt'\n",
        "else:\n",
        "    ckpt = f'{pretrained}.pt'\n",
        "\n",
        "# load base version if preserve_color version not available\n",
        "try:\n",
        "    downloader.download_file(ckpt)\n",
        "except:\n",
        "    ckpt = f'{pretrained}.pt'\n",
        "    downloader.download_file(ckpt)\n",
        "\n",
        "#@title Generate results\n",
        "n_sample =  5#@param {type:\"number\"}\n",
        "seed = 3000 #@param {type:\"number\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKDtUm_3nYCj",
        "outputId": "76406ee1-d446-4359-c4e3-fba6cd6ff0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 1.9G\n",
            "drwxr-xr-x  2 root root 4.0K Aug  2 00:39 .\n",
            "drwxr-xr-x 12 root root 4.0K Aug  2 00:30 ..\n",
            "-rw-r--r--  1 root root 127M Aug  2 00:35 disney_preserve_color.pt\n",
            "-rw-r--r--  1 root root  96M Jul 24  2015 dlibshape_predictor_68_face_landmarks.dat\n",
            "-rw-r--r--  1 root root 1.2G Aug  2 00:31 e4e_ffhq_encode.pt\n",
            "-rw-r--r--  1 root root 127M Aug  2 00:39 sketch_multi.pt\n",
            "-rw-r--r--  1 root root 364M Aug  2 00:31 stylegan2-ffhq-config-f.pt\n"
          ]
        }
      ],
      "source": [
        "!ls -lah /content/JoJoGAN/models/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVF8ZQmhNnaQ",
        "outputId": "c388ad98-e318-47c2-81ea-31248965aeb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ckpt = torch.load(os.path.join('models', ckpt), map_location=lambda storage, loc: storage)\n",
        "generator.load_state_dict(ckpt[\"g\"], strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO6T4FJXngOh",
        "outputId": "48b09fe6-25b9-4445-e6b5-54a81b267eac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/JoJoGAN/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.12.0+cu113. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(seed)\n",
        "with torch.no_grad():\n",
        "    generator.eval()\n",
        "    z = torch.randn(n_sample, latent_dim, device=device)\n",
        "\n",
        "    original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "    sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "\n",
        "    original_my_sample = original_generator(my_w, input_is_latent=True)\n",
        "    my_sample = generator(my_w, input_is_latent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "bWIYDRF7NxMN"
      },
      "outputs": [],
      "source": [
        "# display reference images\n",
        "if pretrained == 'arcane_multi':\n",
        "    style_path = f'style_images_aligned/arcane_jinx.png'\n",
        "elif pretrained == 'sketch_multi':\n",
        "    style_path = f'style_images_aligned/sketch.png'\n",
        "else:   \n",
        "    style_path = f'style_images_aligned/{pretrained}.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5MEIpPx4N0K3"
      },
      "outputs": [],
      "source": [
        "style_image = transform(Image.open(style_path)).unsqueeze(0).to(device)\n",
        "face = transform(aligned_face).unsqueeze(0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "B3X3AT-vN6DH"
      },
      "outputs": [],
      "source": [
        "# #@markdown Upload your own style images into the style_images folder and type it into the field in the following format without the directory name. Upload multiple style images to do multi-shot image translation\n",
        "# names =  ['hulk-01.jpg', 'hulk-04.jpg']#@param {type:\"raw\"}\n",
        "# names =  ['drstrange-01.jpg', 'drstrange-02.jpg', 'drstrange-03.jpg', 'drstrange-04.jpg']#@param {type:\"raw\"}\n",
        "names =  ['chimp.jpg']#@param {type:\"raw\"}\n",
        "targets = []\n",
        "latents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uizrcmPRoZiH"
      },
      "outputs": [],
      "source": [
        "for name in names:\n",
        "    style_path = os.path.join('style_images', name)\n",
        "    assert os.path.exists(style_path), f\"{style_path} does not exist!\"\n",
        "\n",
        "    name = strip_path_extension(name)\n",
        "\n",
        "    # crop and align the face\n",
        "    style_aligned_path = os.path.join('style_images_aligned', f'{name}.png')\n",
        "    if not os.path.exists(style_aligned_path):\n",
        "        style_aligned = align_face(style_path)\n",
        "        style_aligned.save(style_aligned_path)\n",
        "    else:\n",
        "        style_aligned = Image.open(style_aligned_path).convert('RGB')\n",
        "\n",
        "    # GAN invert\n",
        "    style_code_path = os.path.join('inversion_codes', f'{name}.pt')\n",
        "    if not os.path.exists(style_code_path):\n",
        "        latent = e4e_projection(style_aligned, style_code_path, device)\n",
        "    else:\n",
        "        latent = torch.load(style_code_path)['latent']\n",
        "\n",
        "    targets.append(transform(style_aligned).to(device))\n",
        "    latents.append(latent.to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfaxCAJmpwPV",
        "outputId": "2720c4b7-b07c-4072-aa14-440feae63c2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/500 [00:00<?, ?it/s]/content/JoJoGAN/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.12.0+cu113. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n",
            "100%|██████████| 500/500 [03:45<00:00,  2.22it/s]\n"
          ]
        }
      ],
      "source": [
        "#@title Finetune StyleGAN\n",
        "#@markdown alpha controls the strength of the style\n",
        "alpha =  1.0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "alpha = 1-alpha\n",
        "\n",
        "#@markdown Tries to preserve color of original image by limiting family of allowable transformations. Set to false if you want to transfer color from reference image. This also leads to heavier stylization\n",
        "preserve_color = True #@param{type:\"boolean\"}\n",
        "#@markdown Number of finetuning steps. Different style reference may require different iterations. Try 200~500 iterations.\n",
        "num_iter = 500 #@param {type:\"number\"}\n",
        "#@markdown Log training on wandb and interval for image logging\n",
        "use_wandb = False #@param {type:\"boolean\"}\n",
        "log_interval = 50 #@param {type:\"number\"}\n",
        "\n",
        "if use_wandb:\n",
        "    wandb.init(project=\"JoJoGAN\")\n",
        "    config = wandb.config\n",
        "    config.num_iter = num_iter\n",
        "    config.preserve_color = preserve_color\n",
        "    wandb.log(\n",
        "    {\"Style reference\": [wandb.Image(transforms.ToPILImage()(target_im))]},\n",
        "    step=0)\n",
        "\n",
        "# load discriminator for perceptual loss\n",
        "discriminator = Discriminator(1024, 2).eval().to(device)\n",
        "ckpt = torch.load('models/stylegan2-ffhq-config-f.pt', map_location=lambda storage, loc: storage)\n",
        "discriminator.load_state_dict(ckpt[\"d\"], strict=False)\n",
        "\n",
        "# reset generator\n",
        "del generator\n",
        "generator = deepcopy(original_generator)\n",
        "\n",
        "g_optim = optim.Adam(generator.parameters(), lr=2e-3, betas=(0, 0.99))\n",
        "\n",
        "# Which layers to swap for generating a family of plausible real images -> fake image\n",
        "if preserve_color:\n",
        "    id_swap = [9,11,15,16,17]\n",
        "else:\n",
        "    id_swap = list(range(7, generator.n_latent))\n",
        "\n",
        "for idx in tqdm(range(num_iter)):\n",
        "    mean_w = generator.get_latent(torch.randn([latents.size(0), latent_dim]).to(device)).unsqueeze(1).repeat(1, generator.n_latent, 1)\n",
        "    in_latent = latents.clone()\n",
        "    in_latent[:, id_swap] = alpha*latents[:, id_swap] + (1-alpha)*mean_w[:, id_swap]\n",
        "\n",
        "    img = generator(in_latent, input_is_latent=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        real_feat = discriminator(targets)\n",
        "    fake_feat = discriminator(img)\n",
        "\n",
        "    loss = sum([F.l1_loss(a, b) for a, b in zip(fake_feat, real_feat)])/len(fake_feat)\n",
        "    \n",
        "    if use_wandb:\n",
        "        wandb.log({\"loss\": loss}, step=idx)\n",
        "        if idx % log_interval == 0:\n",
        "            generator.eval()\n",
        "            my_sample = generator(my_w, input_is_latent=True)\n",
        "            generator.train()\n",
        "            my_sample = transforms.ToPILImage()(utils.make_grid(my_sample, normalize=True, range=(-1, 1)))\n",
        "            wandb.log(\n",
        "            {\"Current stylization\": [wandb.Image(my_sample)]},\n",
        "            step=idx)\n",
        "\n",
        "    g_optim.zero_grad()\n",
        "    loss.backward()\n",
        "    g_optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsfWuvKep6FW",
        "outputId": "92bb5932-4568-4793-a5cf-2ff0aed7efe5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/JoJoGAN/op/conv2d_gradfix.py:89: UserWarning: conv2d_gradfix not supported on PyTorch 1.12.0+cu113. Falling back to torch.nn.functional.conv2d().\n",
            "  f\"conv2d_gradfix not supported on PyTorch {torch.__version__}. Falling back to torch.nn.functional.conv2d().\"\n"
          ]
        }
      ],
      "source": [
        "#@title Generate results\n",
        "n_sample =  5#@param {type:\"number\"}\n",
        "seed = 3000 #@param {type:\"number\"}\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "with torch.no_grad():\n",
        "    generator.eval()\n",
        "    z = torch.randn(n_sample, latent_dim, device=device)\n",
        "\n",
        "    original_sample = original_generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "    sample = generator([z], truncation=0.7, truncation_latent=mean_latent)\n",
        "\n",
        "    original_my_sample = original_generator(my_w, input_is_latent=True)\n",
        "    my_sample = generator(my_w, input_is_latent=True)\n",
        "\n",
        "# display reference images\n",
        "style_images = []\n",
        "for name in names:\n",
        "    style_path = f'style_images_aligned/{strip_path_extension(name)}.png'\n",
        "    style_image = transform(Image.open(style_path))\n",
        "    style_images.append(style_image)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "JoJoGAN-Exercise.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
